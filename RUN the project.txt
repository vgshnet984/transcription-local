================================================================================
                    LOCAL TRANSCRIPTION PLATFORM - RUN GUIDE
================================================================================

This guide provides step-by-step instructions for running the transcription platform
in different modes and configurations.

================================================================================
                                PREREQUISITES
================================================================================

1. Python 3.9+ installed
2. FFmpeg installed and in PATH
3. CUDA-compatible GPU (optional, for faster processing)
4. Hugging Face token (for speaker diarization)
5. cuDNN libraries in PATH (C:\cudnn\bin) - REQUIRED for GPU acceleration
6. **NEW TERMINAL REQUIRED** - Always open a fresh Windows PowerShell terminal before running

================================================================================
                                QUICK START (NEW TERMINAL REQUIRED)
================================================================================

**IMPORTANT: Always open a NEW Windows PowerShell terminal before running!**

1. Open new **Windows PowerShell** terminal (not Anaconda PowerShell)
2. Navigate to project: `cd C:\Users\varad\transcription-local-optimized`
3. Activate environment: `conda activate transcription-env`
4. Set cuDNN PATH: `$env:PATH += ";C:\cudnn\bin"`
5. Start server: `python -m src.main`
6. Open browser: http://localhost:8000

================================================================================
                                INITIAL SETUP
================================================================================

CUDA/cuDNN SETUP (REQUIRED FOR GPU ACCELERATION)
   ```powershell
   # 1. Ensure cuDNN is in PATH (add to system PATH permanently)
   $env:PATH += ";C:\cudnn\bin"
   
   # 2. Verify cuDNN is accessible
   echo $env:PATH | findstr cudnn
   
   # 3. Test CUDA availability
   python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
   ```

1. ACTIVATE CONDA ENVIRONMENT (NEW SETUP)
   ```powershell
   # Windows PowerShell (Anaconda) - OPEN NEW TERMINAL FIRST!
   conda activate transcription-env
   ```

2. VERIFY INSTALLATION
   ```bash
   # Check CUDA and PyTorch
   python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('PyTorch CUDA version:', torch.version.cuda); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU')"
   
   # Check transcription engines
   python -c "import whisper; print('Whisper OK')"
   python -c "import faster_whisper; print('Faster-Whisper OK')"
   
   # Check cuDNN PATH (IMPORTANT!)
   echo $env:PATH | findstr cudnn
   ```

3. DOWNLOAD MODELS (OPTIONAL - AUTO-DOWNLOADED ON FIRST USE)
   ```bash
   # Download quantized models for faster-whisper
   python scripts/download_quantized_model.py --model large-v3 --compute-type float16
   ```

================================================================================
                                WEB INTERFACE (RECOMMENDED)
================================================================================

1. START THE SERVER
   ```bash
   # Navigate to project directory
   cd C:\Users\varad\transcription-local-optimized
   
   # IMPORTANT: Set cuDNN PATH for GPU acceleration
   $env:PATH += ";C:\cudnn\bin"
   
   # Start the web server
   python -m src.main
   ```

2. ACCESS THE INTERFACE
   - Open browser: http://localhost:8000
   - Upload audio files via drag-and-drop or click "Choose File"
   - Configure transcription settings:
     * Engine: whisper, whisperx, or fast-whisper (default)
     * Model: tiny, base, small, medium, large, large-v3
     * Language: auto-detect or specific language
     * Single Speaker: Check for monologues/lectures (faster)
     * Preserve Speakers: Check for multi-speaker streaming
   - Click "Start Transcription"
   - Monitor real-time progress
   - Download results

3. WEB INTERFACE FEATURES
   - Drag-and-drop file upload
   - Real-time progress tracking
   - Multiple engine support (whisper, whisperx, fast-whisper)
   - Single speaker mode for faster processing
   - Speaker preservation across chunks
   - Download results in multiple formats
   - Speaker diarization visualization

4. STOP THE SERVER
   - Press Ctrl + C in terminal
   - Or use: taskkill /f /im python.exe (if server hangs)

================================================================================
                                COMMAND LINE INTERFACE
================================================================================

1. ULTRA-FAST CLI (RECOMMENDED - NEW!)
   ```bash
   # Basic usage with faster-whisper
   python scripts/transcribe_cli_ultra_fast.py --input "audio.wav"
   
   # Single speaker mode (fastest for monologues/lectures)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --single-speaker \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   
   # Multi-speaker with speaker preservation
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --preserve-speakers \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   
   # Streaming with chunked processing
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --stream \
       --chunk-size 10 \
       --preserve-speakers \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   ```

2. FAST CLI (NO DATABASE)
   ```bash
   # Basic usage
   python scripts/transcribe_cli_fast.py --input "audio.wav"
   
   # With specific settings
   python scripts/transcribe_cli_fast.py \
       --input "audio.wav" \
       --language en \
       --engine whisperx \
       --model large-v3 \
       --device cuda \
       --speaker-diarization \
       --output "result.txt"
   ```

3. FULL CLI (WITH DATABASE)
   ```bash
   # Basic usage
   python scripts/transcribe_cli.py --input "audio.wav"
   
   # With all options
   python scripts/transcribe_cli.py \
       --input "audio.wav" \
       --language en \
       --engine whisperx \
       --model large-v3 \
       --device cuda \
       --speaker-diarization \
       --output "result.txt"
   ```

================================================================================
                                COMMON USE CASES
================================================================================

1. FAST MONOLOGUE TRANSCRIPTION (SINGLE SPEAKER)
   ```bash
   # Fastest for lectures, podcasts, solo recordings
   python scripts/transcribe_cli_ultra_fast.py \
       --input "lecture.wav" \
       --single-speaker \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda \
       --output "lecture_transcript.txt"
   ```

2. MULTI-SPEAKER MEETING TRANSCRIPTION
   ```bash
   # With speaker preservation across chunks
   python scripts/transcribe_cli_ultra_fast.py \
       --input "meeting.wav" \
       --preserve-speakers \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda \
       --output "meeting_with_speakers.txt"
   ```

3. ENGLISH TRANSCRIPTION
   ```bash
   # Fast English transcription
   python scripts/transcribe_cli_ultra_fast.py \
       --input "examples/sample_audio/tes1.mp3" \
       --language en \
       --model large-v3 \
       --device cuda \
       --output "transcript_output/english_result.txt"
   ```

4. TAMIL TRANSCRIPTION
   ```bash
   # Tamil with native script
   python scripts/transcribe_cli_ultra_fast.py \
       --input "examples/sample_audio/tamil_test.mp3" \
       --language ta \
       --model large-v3 \
       --device cuda \
       --output "transcript_output/tamil_native.txt"
   
   # Tamil with romanized text
   python scripts/transcribe_cli_ultra_fast.py \
       --input "examples/sample_audio/tamil_test.mp3" \
       --language ta \
       --model large \
       --device cuda \
       --romanized \
       --output "transcript_output/tamil_romanized.txt"
   ```

5. SANSKRIT TRANSCRIPTION
   ```bash
   # Sanskrit with Devanagari script
   python scripts/transcribe_cli_ultra_fast.py \
       --input "examples/sample_audio/sanskrit_audio.wav" \
       --language sa \
       --model large \
       --device cuda \
       --output "transcript_output/sanskrit_devanagari.txt"
   ```

6. SPEAKER DIARIZATION
   ```bash
   # With WhisperX and speaker diarization
   python scripts/transcribe_cli_ultra_fast.py \
       --input "examples/sample_audio/tes1.mp3" \
       --language en \
       --engine whisperx \
       --model large-v3 \
       --device cuda \
       --preserve-speakers \
       --output "transcript_output/with_speakers.txt"
   ```

================================================================================
                                PERFORMANCE OPTIMIZATION
================================================================================

1. ULTRA-FAST OPTIMIZATION (FASTEST - NEW!)
   ```bash
   # Use faster-whisper for maximum speed (4-6x faster than standard Whisper)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda \
       --compute-type float16 \
       --cpu-threads 8

   # Batch processing for multiple files
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio_folder/" \
       --batch \
       --batch-size 8 \
       --max-workers 4 \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda

   # Streaming with speaker preservation (two-pass approach, best for multi-speaker audio)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --stream \
       --preserve-speakers \
       --chunk-size 10 \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda

   # Fastest monologue/lecture mode (skip diarization, single speaker)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --stream \
       --single-speaker \
       --chunk-size 10 \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   ```

2. MEMORY OPTIMIZATION
   ```bash
   # Use quantized models for lower memory usage
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model large-v3 \
       --compute-type int8 \
       --device cuda

   # CPU-only processing for low-memory systems
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model base \
       --device cpu \
       --cpu-threads 4
   ```

3. SPEED VS ACCURACY TRADE-OFFS
   ```bash
   # Fastest (lower accuracy)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model tiny \
       --single-speaker \
       --device cuda

   # Balanced (good speed and accuracy)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model medium \
       --device cuda

   # Highest accuracy (slower)
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio.wav" \
       --engine fast-whisper \
       --model large-v3 \
       --preserve-speakers \
       --device cuda
   ```

================================================================================
                                API USAGE
================================================================================

1. REST API ENDPOINTS
   ```bash
   # Upload file
   curl -X POST -F "file=@audio.wav" http://localhost:8000/api/upload
   
   # Start transcription
   curl -X POST http://localhost:8000/api/transcribe \
       -H "Content-Type: application/json" \
       -d '{
           "file_id": "file_id_from_upload",
           "engine": "fast-whisper",
           "model": "large-v3",
           "language": "en",
           "single_speaker": true,
           "preserve_speakers": false
       }'
   
   # Get transcription results
   curl http://localhost:8000/api/transcriptions/{transcription_id}
   ```

2. WEBSOCKET STREAMING
   ```bash
   # Connect to WebSocket for real-time streaming
   python test_websocket_client.py --file "audio.wav" --single-speaker
   ```

================================================================================
                                TROUBLESHOOTING
================================================================================

1. COMMON ISSUES
   - "No module named uvicorn": Install with `pip install uvicorn`
   - "No module named src": Run with `python -m src.main`
   - CUDA out of memory: Use smaller model or `--compute-type int8`
   - Slow processing: Use `--single-speaker` for monologues

2. PERFORMANCE TIPS
   - Use `--single-speaker` for lectures, podcasts, solo recordings
   - Use `--preserve-speakers` for meetings, interviews, multi-speaker content
   - Use `fast-whisper` engine for best speed/accuracy balance
   - Use quantized models (`--compute-type int8`) for lower memory usage
   - Use streaming (`--stream`) for long audio files

3. MEMORY OPTIMIZATION
   - Use smaller models (tiny, base, small) for low-memory systems
   - Use CPU processing if GPU memory is insufficient
   - Use quantized models with `--compute-type int8`
   - Process files in chunks with `--stream --chunk-size 10`

================================================================================
                        TROUBLESHOOTING: cuDNN DLL ERRORS
================================================================================

If you see an error like:
    Could not locate cudnn_cnn_infer64_8.dll. Please make sure it is in your library path!

Do the following:

1. Make sure C:\cudnn\bin exists and contains cudnn_cnn_infer64_8.dll
2. Before starting the server, set your PATH in the shell:
   ```powershell
   $env:PATH += ";C:\cudnn\bin"
   python -m src.main
   ```
3. To make this permanent (recommended):
   - Open Windows Start Menu, search for "Environment Variables"
   - Edit your user PATH
   - Add: C:\cudnn\bin
   - Click OK and restart your shell

This ensures all processes (including the web server) can find the cuDNN DLLs for GPU acceleration.

================================================================================
                                FEATURE COMPARISON
================================================================================

| Feature                    | Ultra-Fast CLI | Fast CLI | Full CLI | Web UI |
|---------------------------|----------------|----------|----------|--------|
| Speed                     | ⭐⭐⭐⭐⭐      | ⭐⭐⭐⭐   | ⭐⭐⭐    | ⭐⭐⭐⭐ |
| Memory Usage              | ⭐⭐⭐⭐⭐      | ⭐⭐⭐⭐   | ⭐⭐⭐    | ⭐⭐⭐⭐ |
| Single Speaker Mode       | ✅             | ❌       | ❌       | ✅     |
| Speaker Preservation      | ✅             | ❌       | ❌       | ✅     |
| Streaming                 | ✅             | ❌       | ❌       | ✅     |
| Batch Processing          | ✅             | ❌       | ❌       | ❌     |
| Database Storage          | ❌             | ❌       | ✅       | ✅     |
| Web Interface             | ❌             | ❌       | ❌       | ✅     |
| Real-time Progress        | ❌             | ❌       | ❌       | ✅     |

================================================================================
                                RECOMMENDED WORKFLOWS
================================================================================

1. FAST MONOLOGUE TRANSCRIPTION
   ```bash
   python scripts/transcribe_cli_ultra_fast.py \
       --input "lecture.wav" \
       --single-speaker \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   ```

2. MULTI-SPEAKER MEETING TRANSCRIPTION
   ```bash
   python scripts/transcribe_cli_ultra_fast.py \
       --input "meeting.wav" \
       --preserve-speakers \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   ```

3. BATCH PROCESSING
   ```bash
   python scripts/transcribe_cli_ultra_fast.py \
       --input "audio_folder/" \
       --batch \
       --single-speaker \
       --engine fast-whisper \
       --model large-v3 \
       --device cuda
   ```

4. WEB INTERFACE (USER-FRIENDLY)
   - Start server: `python -m src.main`
   - Open browser: http://localhost:8000
   - Upload file and configure settings
   - Click "Start Transcription"

================================================================================
                                DEPLOYMENT OPTIONS
================================================================================

1. LOCAL DEVELOPMENT (RECOMMENDED)
   ```bash
   python -m src.main
   ```

2. DOCKER (OPTIONAL)
   ```bash
   docker-compose -f docker-compose.local.yml up
   ```

3. PRODUCTION (OPTIONAL)
   ```bash
   # Use gunicorn for production
   pip install gunicorn
   gunicorn src.main:app -w 4 -k uvicorn.workers.UvicornWorker
   ```

================================================================================
                                SUPPORTED LANGUAGES
================================================================================

- English (en) - Default
- Tamil (ta) - Native script and romanized
- Sanskrit (sa) - Devanagari script
- Hindi (hi)
- Spanish (es)
- French (fr)
- German (de)
- And 90+ other languages supported by Whisper

================================================================================
                                MODEL COMPARISON
================================================================================

| Model     | Size  | Speed | Accuracy | Memory | Use Case |
|-----------|-------|-------|----------|--------|----------|
| tiny      | 39MB  | ⭐⭐⭐⭐⭐ | ⭐⭐     | ⭐⭐⭐⭐⭐ | Fast preview |
| base      | 74MB  | ⭐⭐⭐⭐ | ⭐⭐⭐   | ⭐⭐⭐⭐ | Quick transcription |
| small     | 244MB | ⭐⭐⭐  | ⭐⭐⭐⭐ | ⭐⭐⭐  | Balanced |
| medium    | 769MB | ⭐⭐   | ⭐⭐⭐⭐⭐ | ⭐⭐   | High accuracy |
| large     | 1550MB| ⭐    | ⭐⭐⭐⭐⭐ | ⭐    | Best accuracy |
| large-v3  | 1550MB| ⭐    | ⭐⭐⭐⭐⭐ | ⭐    | Latest & best |

================================================================================
                                ENGINE COMPARISON
================================================================================

| Engine        | Speed | Accuracy | Memory | Features |
|---------------|-------|----------|--------|----------|
| whisper       | ⭐⭐⭐  | ⭐⭐⭐⭐⭐ | ⭐⭐⭐  | Standard |
| whisperx      | ⭐⭐   | ⭐⭐⭐⭐⭐ | ⭐⭐   | Enhanced diarization |
| fast-whisper  | ⭐⭐⭐⭐⭐| ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐| Optimized, streaming |

================================================================================
                                END
================================================================================

For more information, see:
- README.md: Project overview
- architecture.md: System architecture
- audio_processing.md: Audio processing details
- speaker_identification.md: Speaker diarization
- api_specification.md: API documentation

# How to Run the Transcription Platform (Basic UI + Scripflow UI)

## Prerequisites
- Python 3.9+ (recommend using Miniconda or Mambaforge)
- All dependencies installed: `pip install -r requirements_local.txt`
- FFmpeg installed and in your PATH (for audio processing)
- (Optional) CUDA drivers for GPU acceleration
- (Optional) Install extra dependencies for advanced features: `pip install webrtcvad pyannote.audio` etc.

## 1. Setup Environment
- Run the setup script to create necessary folders and set UTF-8 encoding:

    ```sh
    python scripts/setup.sh
    # or on Windows:
    scripts\setup_huggingface.py
    ```

- Make sure the following directories exist:
    - `uploads/` (for audio uploads)
    - `transcripts/` (for transcription outputs)
    - `models/` (for downloaded models)

## 2. Setup CUDA/cuDNN for GPU Acceleration (Optional but Recommended)
```sh
# Download and setup cuDNN for GPU acceleration
python scripts/download_cudnn.py

# Verify CUDA setup
python scripts/verify_cuda_setup.py
```

**Note**: If you don't have an NVIDIA GPU or don't want GPU acceleration, skip this step. The platform will work with CPU-only mode.

## 3. Set HuggingFace Token (for Speaker Diarization)
```sh
python set_token.py YOUR_HF_TOKEN_HERE
```

## 4. Quick Start Commands

### Option A: Start Basic UI Only
```sh
python start_basic_ui.py
```
- **URL:** http://localhost:8000
- **Features:** Simple upload, transcription, download
- **Best for:** Quick transcriptions, basic usage

### Option B: Start Scripflow UI Only  
```sh
python start_scripflow.py
```
- **URL:** http://localhost:8001
- **Features:** Advanced interface, real-time progress, detailed controls
- **Best for:** Professional use, batch processing

### Option C: Start Both UIs (Recommended)
```sh
# Terminal 1 - Basic UI
python start_basic_ui.py

# Terminal 2 - Scripflow UI  
python start_scripflow.py
```

## 4. Usage

### Basic UI (http://localhost:8000)
1. Upload audio file (WAV, MP3, M4A, FLAC)
2. Select transcription options:
   - Language (English, Hindi, Tamil, etc.)
   - Engine (Whisper, Faster-Whisper, WhisperX)
   - Model size (tiny, base, small, medium, large)
   - Enable speaker diarization (if token is set)
3. Click "Start Transcription"
4. Download results when complete

### Scripflow UI (http://localhost:8001)
1. Upload audio file
2. Configure advanced settings:
   - Multiple engine options
   - Real-time progress tracking
   - Speaker identification
   - Audio preprocessing options
3. Monitor progress in real-time
4. Export results in multiple formats

## 5. CLI Usage (Alternative)
```sh
# Quick transcription
python scripts/transcribe_cli.py uploads/your_audio.wav

# Fast transcription with specific model
python scripts/transcribe_cli_fast.py uploads/your_audio.wav --model base

# Ultra-fast transcription
python scripts/transcribe_cli_ultra_fast.py uploads/your_audio.wav
```

## 6. Stop Servers
- Press `Ctrl+C` in each terminal
- Or use the batch script: `stop_servers.bat`

## 7. Troubleshooting

### Common Issues:
- **"No module named 'src'"**: Make sure you're in the project root directory
- **"HF_TOKEN not set"**: Run `python set_token.py <your_token>`
- **"pyannote.audio not available"**: Install with `pip install pyannote.audio`
- **"Could not locate cudnn_cnn_infer64_8.dll"**: Ensure CUDA/cuDNN is in PATH

### Verify Setup:
```sh
# Test diarization setup
python test_diarization.py

# Test basic functionality
python test_basic_ui.py

# Test both UIs
python test_both_uis_enhanced.py
```

### Log Files:
- Check `logs/` directory for detailed logs
- Basic UI logs: Console output (minimal)
- Scripflow logs: Console output (detailed)

## 8. Advanced Configuration

### Environment Variables:
```sh
# Set in your shell or .env file
export HF_TOKEN="your_huggingface_token"
export CUDA_VISIBLE_DEVICES="0"  # Use specific GPU
export PYTHONPATH="."
```

### Model Management:
```sh
# Download models
python scripts/download_models.py

# Test model availability
python scripts/test_models.py
```

## 9. Performance Tips

### For Fast Processing:
- Use "faster-whisper" engine
- Select smaller models (tiny, base)
- Enable GPU if available
- Use audio preprocessing for noisy files

### For High Accuracy:
- Use "whisperx" engine
- Select larger models (medium, large)
- Enable speaker diarization
- Use audio preprocessing

### For Speaker Diarization:
- Ensure HF_TOKEN is set
- Use longer audio files (>30 seconds)
- Multiple speakers in audio
- Clear audio quality

## 10. File Locations
- **Uploads:** `uploads/` directory
- **Transcripts:** `transcripts/` directory  
- **Models:** `models/` directory
- **Logs:** `logs/` directory
- **Database:** `transcription.db` (SQLite)

## Quick Reference
| Command | Purpose | URL |
|---------|---------|-----|
| `python start_basic_ui.py` | Start Basic UI | http://localhost:8000 |
| `python start_scripflow.py` | Start Scripflow UI | http://localhost:8001 |
| `python test_diarization.py` | Test diarization | - |
| `python set_token.py <token>` | Set HF token | - |

**Note:** Both UIs share the same backend models and database, so you can switch between them seamlessly.

